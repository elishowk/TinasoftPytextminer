---
# application main params
general:
    # where all path and following file are located
    basedirectory: '.'
    locale: ''
    # following directories or files a under "basedirectory"
    index: 'index'
    dbenv: 'db'
    log: 'log'
    loglevel: 'debug'
    user: 'user'
    shared: 'shared'
    source_file_directory: 'source_files'
    userstopwords: 'user_stopwords.csv'
    # stopwords are under share
    stopwords: 'stopwords\en.txt'
# extraction settings
datasets:
    doc_extraction:
        # the following values can be document object fields:
        # - defined by one of the field's value
        # - constants required fields : 'content'/'label'/'id'
        - 'content'
        - 'label'
    # tina csv columns declaration
    # will ignore undeclared fields
    # and warn not found optional fields
    tinacsv:
        # doc_label represents one of the field's key
        doc_label: 'label'
        fields:
            # required fields
            label: 'title'
            content: 'abstract'
            corpus_id: 'corp_id'
            id: 'doc_id'
            # optionnal fields
            #author: 'acronym'
        # csv reader params
        #delimiter: ','
        #quotechar: '"'
        encoding: 'utf-8'
        dialect: 'excel'
    # pubmed.gov "medline" file export
    # same behaviour as tinacsv
    # period_size = number first characters of pub date field "corpusField"
    medline:
        doc_label: 'id'
        period_size: 4
        encoding: 'ascii'
        fields:
            label: 'TI'
            content: 'AB'
            corpus_id: 'DP'
            id: 'PMID'
    # archive of pubmed.gov "medline" files, organized like this
    # Medline/
    #    period1/period1.txt
    #    period2/period2.txt
    #medlinearchive:
        # nothing yet
    # extraction params
    ngramMin: 1
    ngramMax: 4
    # tagger learning on 2 x training_tagger_size sentences of tagged nltk corpus
    training_tagger_size: 8000
    # under "basedirectory" directory, delete this file to regenerate a new one on next starting
    tagger: 'shared_tagger.pickle'
    postag_valid: '^((VB,|VBD,|VBG,|VBN,|CD.?,|JJ.?,|\?,){0,2}?(N.?,|\?,))+?((PREP.?|DET.?,|IN.?,|CC.?,|\?,)((VB,|VBD,|VBG,|VBN,|CD.?,|JJ.?,|\?,){0,2}?(N.?,|\?,))+?)*?$'
    #postag_valid: '^((VB.?,|CD.?,|JJ.?,|\?,){0,2}?(N.?,|\?,))+?((PREP.?|DET.?,|IN.?,|CC.?,|\?,)((VB.?,|CD.?,|JJ.?,|\?,){0,2}?(N.?,|\?,))+?)*?$'
    #postag_valid: '^((VB.?,|JJ.?,|\?,){0,2}?(N.?,|\?,))+?((IN.?,|CC.?,|\?,)((VB.?,|JJ.?,|\?,){0,2}?(N.?,|\?,))+?)*?$'
datamining:
    template: 'shared/gexf/gexf.default.template'
    # default values if no params are passed to gexf exporter
    DocumentGraph:
        edgethreshold:
            - 0.0001
            - 'inf'
        nodethreshold:
            - 1.00
            - 'inf'
        proximity: 'sharedNGrams'
        #proximity: 'logJaccard'
    NGramGraph:
        edgethreshold:
            - 0.0001
            - 'inf'
        nodethreshold:
            - 1.00
            - 'inf'
        alpha: 0.10
        #hapax: 1
        proximity: 'Cooccurrences'
        #proximity: 'EquivalenceIndex'
        #proximity: 'PseudoInclusion'
